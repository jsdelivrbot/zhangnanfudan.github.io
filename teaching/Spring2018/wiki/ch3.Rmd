---
title: "Chapter 3: ARIMA Models"
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cached=TRUE)
library(astsa)
```

## 1. Autoregressive Moving Average Models
* AR, MA and ARMA models
* Backshift operator, mean square convergence
* Causal and explosive, stochastically equivalent
* Nonuniqueness and invertibility
* Matching, Taylor expansion for functions of complex variables
* Causal and invertible conditions for ARMA, parameter redundancy

## 2. Difference Equations
* General homogeneous difference Equations
* General solution
* ACF of AR(2), $\psi$ weights for ARMA

## 3. Autocorrelation and Partial Autocorrelation
* Definition and motivation
* Basics of regression
* Partial correlation and conditional correlation
* ACF and PACF for AR and MA
* Interpret the ACF and PACF plots

## 4. Forecasting
* Mean square loss function and conditional mean
* Linear predictor and best linear predictor
* Prediction equations: derivation and matrix form, mean square prediction error
* The Durbin-Levinson algorithm: induction proof and projection proof
* The innovation algorithm: idea and comparison with the Durbin-Levinson
* Forecasting ARMA: forecasts based on finite and infinite past, truncated prediction 

## 5. Estimation
* Method of moments: Yule-Walker equations, the Durbin-Levinson algorithm
* MLE and least square estimation: unconditional and conditional versions
* Large sample properties

## 6. Integrated Models for Nonstationary Data
* Trend model: deterministic and stochastic trends
* ARIMA models
* Forecasting


## Code

### Example 3.2 AR(p)
```{r arp}
par(mfrow=c(2,1))                         
# in the expressions below, ~ is a space and == is equal
tsplot(arima.sim(list(order=c(1,0,0), ar=.9), n=100), ylab="x", main=(expression(AR(1)~~~phi==+.9))) 
tsplot(arima.sim(list(order=c(1,0,0), ar=-.9), n=100), ylab="x", main=(expression(AR(1)~~~phi==-.9))) 
```


### Example 3.5 MA(1)
```{r ma1}
par(mfrow=c(2,1))                                   
tsplot(arima.sim(list(order=c(0,0,1), ma=.9), n=100), ylab="x", main=(expression(MA(1)~~~theta==+.9)))    
tsplot(arima.sim(list(order=c(0,0,1), ma=-.9), n=100), ylab="x", main=(expression(MA(1)~~~theta==-.9)))    
```


### Example 3.11 AR(2) with complex roots
```{r ar2}
par(mfrow=c(1,3))
ACF = ARMAacf(ar=c(1,-.25), ma=0, 50)
plot(ACF, type="h", xlab="lag", main="equal real roots")
abline(h=0)

ACF = ARMAacf(ar=c(.75,-.125), ma=0, 50)
plot(ACF, type="h", xlab="lag", main="distinct real roots")
abline(h=0)

ACF = ARMAacf(ar=c(1.5,-.75), ma=0, 50)
plot(ACF, type="h", xlab="lag", main="complex roots")
abline(h=0)
```


### Example 3.16 ACF and PACF of AR(p)
```{r acf-pacf}
ar2.acf = ARMAacf(ar=c(1.5,-.75), ma=0, 24)[-1]
ar2.pacf = ARMAacf(ar=c(1.5,-.75), ma=0, 24, pacf=TRUE)
par(mfrow=c(1,2))
plot(ar2.acf, type="h", xlab="lag")
abline(h=0)
plot(ar2.pacf, type="h", xlab="lag")
abline(h=0)
```


### Example 3.18 Preliminary analysis of Rec
```{r rec}
tsplot(rec, ylab="", main="Recruitment") 
acf2(rec, 48)     # will produce values and a graphic 
(regr = ar.ols(rec, order=2, demean=F, intercept=TRUE))  # regression
regr$asy.se.coef  # standard errors                             
```



### Example 3.25 Forecasting the Recruitment Series
```{r rec-forecasting, warning=FALSE}
regr = ar.ols(rec, order=2, demean=FALSE, intercept=TRUE)
fore = predict(regr, n.ahead=24)
ts.plot(rec, fore$pred, col=1:2, xlim=c(1980,1990), ylab="Recruitment")
lines(fore$pred, type="p", col=2)
lines(fore$pred+fore$se, lty="dashed", col=4)
lines(fore$pred-fore$se, lty="dashed", col=4)
```


### Example 3.28 Yule-Walker Estimation of the Recruitment Series
```{r rec-yw}
rec.yw = ar.yw(rec, order=2)
rec.yw$x.mean  # = 62.26278 (mean estimate)
rec.yw$ar      # = 1.3315874, -.4445447  (parameter estimates)
sqrt(diag(rec.yw$asy.var.coef))  # = .04222637, .04222637  (standard errors)
rec.yw$var.pred  # = 94.79912 (error variance estimate)

rec.pr = predict(rec.yw, n.ahead=24)
U = rec.pr$pred + rec.pr$se
L = rec.pr$pred - rec.pr$se
minx = min(rec,L); maxx = max(rec,U)
ts.plot(rec, rec.pr$pred, xlim=c(1980,1990), ylim=c(minx,maxx)) 
lines(rec.pr$pred, col="red", type="o")
lines(U, col="blue", lty="dashed")
lines(L, col="blue", lty="dashed")
```


### Example 3.31 MLE for the Recruitment Series
```{r rec-mle}
rec.mle = ar.mle(rec, order=2)
rec.mle$x.mean
rec.mle$ar
sqrt(diag(rec.mle$asy.var.coef))
rec.mle$var.pred

rec.pr = predict(rec.mle, n.ahead=24)
U = rec.pr$pred + rec.pr$se
L = rec.pr$pred - rec.pr$se
minx = min(rec,L); maxx = max(rec,U)
ts.plot(rec, rec.pr$pred, xlim=c(1980,1990), ylim=c(minx,maxx)) 
lines(rec.pr$pred, col="red", type="o")
lines(U, col="blue", lty="dashed")
lines(L, col="blue", lty="dashed")
```


### Example 3.33 Fitting the Glacial Varve Series
```{r varve}
x = diff(log(varve))
# Evaluate Sc on a Grid
c(0) -> w -> z
c() -> Sc -> Sz -> Szw
num = length(x)
th = seq(-.3,-.94,-.01)
for (p in 1:length(th)){
  for (i in 2:num){ w[i] = x[i]-th[p]*w[i-1] }
  Sc[p] = sum(w^2) }
plot(th, Sc, type="l", ylab=expression(S[c](theta)), xlab=expression(theta),
     lwd=2)
# Gauss-Newton Estimation
r = acf(x, lag=1, plot=FALSE)$acf[-1]
rstart = (1-sqrt(1-4*(r^2)))/(2*r) # from (3.105)
c(0) -> w -> z
c() -> Sc -> Sz -> Szw -> para
niter = 12
para[1] = rstart
for (p in 1:niter){
  for (i in 2:num){ w[i] = x[i]-para[p]*w[i-1]
  z[i] = w[i-1]-para[p]*z[i-1] }
  Sc[p] = sum(w^2)
  Sz[p] = sum(z^2)
  Szw[p] = sum(z*w)
  para[p+1] = para[p] + Szw[p]/Sz[p] }

round(cbind(iteration=0:(niter-1), thetahat=para[1:niter] , Sc , Sz ), 3)
abline(v = para[1:12], lty=2)
points(para[1:12], Sc[1:12], pch=16)
```


### Example 3.38 IMA(1,1) and EWMA
```{r ima}
set.seed(666)    
x = arima.sim(list(order = c(0,1,1), ma = -0.8), n = 100)
(x.ima = HoltWinters(x, beta=FALSE, gamma=FALSE))  # α is 1-λ here
plot(x.ima)
```
